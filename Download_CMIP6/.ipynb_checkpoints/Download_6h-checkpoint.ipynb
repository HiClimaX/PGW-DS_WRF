{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c4fe5-55bd-4fda-a8f2-7c3874d2d4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63679b9-8aa5-40ec-af02-1d86452529ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intake_esm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintake\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintake_esm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'intake_esm'"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862884b6-92db-4a09-b358-a9f4fb2afae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/NCAR/intake-esm-datastore/master/catalogs/pangeo-cmip6.json\"\n",
    "# open the catalog\n",
    "dataframe = intake.open_esm_datastore(url)\n",
    "dataframe.df.columns\n",
    "df = dataframe.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e798a33-6051-4272-9740-1bd26304c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93778d6c-7b0a-4806-b84c-3b3b39c4fad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tas historical EC-Earth3 73\n",
      "r1i1p1f1\n",
      "Download\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "write to  Download_s8/EC-Earth3/historical/tas_CMIP.EC-Earth-Consortium.EC-Earth3.historical.Amon.gr_r1i1p1f1.nc\n",
      "ta historical EC-Earth3 71\n",
      "r1i1p1f1\n",
      "Download\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "write to  Download_s8/EC-Earth3/historical/ta_CMIP.EC-Earth-Consortium.EC-Earth3.historical.Amon.gr_r1i1p1f1.nc\n",
      "ua historical EC-Earth3 72\n",
      "r1i1p1f1\n",
      "Download\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "write to  Download_s8/EC-Earth3/historical/ua_CMIP.EC-Earth-Consortium.EC-Earth3.historical.Amon.gr_r1i1p1f1.nc\n"
     ]
    }
   ],
   "source": [
    "for sid in [ 'EC-Earth3', 'MIROC6',  'MRI-ESM2-0', 'ACCESS-CM2', 'IPSL-CM6A-LR', 'MPI-ESM1-2-HR'][:1]:\n",
    "    for exp in ['historical','ssp585', 'ssp126', 'ssp370','ssp245'][:]:\n",
    "            for var in ['tas','ta','ua','va','hur','zg','ts'][:]:\n",
    "                # seach all files with information given above\n",
    "                models = dataframe.search(experiment_id=exp,\n",
    "                                          table_id='Amon',\n",
    "                                          variable_id=var,\n",
    "                                          source_id = sid,\n",
    "                                          #institution_id=ins,\n",
    "                                          #member_id=mem\n",
    "                                          )  \n",
    "                # then one might get several files with the same conditions\n",
    "                # r1: Realization (initial condition run)\n",
    "                # i1: Initialization method\n",
    "                # p1: Physical parameters\n",
    "                # f1: External forcings\n",
    "\n",
    "                print(var, exp, sid,  len(models.df))\n",
    "\n",
    "                # if no files exist then print out error\n",
    "                if len(models.df) == 0: print('*** \\n Erorrrr \\n')\n",
    "\n",
    "                # sort the possible files\n",
    "                ml = natural_sort(models.df.member_id.values)\n",
    "\n",
    "                # get the first one only then seach again\n",
    "                mem = ml[0]\n",
    "                model_s = dataframe.search(experiment_id=exp,\n",
    "                                          table_id='Amon',\n",
    "                                          variable_id=var,\n",
    "                                          source_id = sid,\n",
    "                                          #institution_id=ins,\n",
    "                                          member_id=mem\n",
    "                                          )                  \n",
    "\n",
    "                # if no files exist then print out error\n",
    "                if len(model_s.df) == 0: print('*** \\n Erorrrr \\n')\n",
    "\n",
    "                print(mem)\n",
    "\n",
    "                if len(model_s.df) > 0:\n",
    "                    print('Download')\n",
    "                    \n",
    "                    if True:\n",
    "                        \n",
    "                        try:\n",
    "                        \n",
    "                            datasets = model_s.to_dataset_dict(zarr_kwargs={'consolidated': True, \"decode_times\": True, \"use_cftime\": True })\n",
    "                            #datasets = models.to_dataset_dict(xarray_open_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True})\n",
    "                            \n",
    "                            for k, v in datasets.items():\n",
    "                                odir = 'Download_s8/'+sid+'/'+exp+'/'\n",
    "                                if not os.path.exists(odir): os.makedirs(odir)\n",
    "                                ofile = odir + var + '_'+ k + '_'+mem+'.nc'\n",
    "                                print('write to ',ofile)\n",
    "                                v.to_netcdf(ofile)\n",
    "                        except:\n",
    "                            print('fail')\n",
    "                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
